{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b510a37-f3e7-4125-8cff-c294bb4a06da",
   "metadata": {},
   "source": [
    "# What is Optimization in Deep Learning?\n",
    "\n",
    "Optimization in deep learning refers to methods used to minimize the loss function by updating model parameters (weights & biases).\n",
    "\n",
    "minâ€‹L(Î¸)\n",
    "\n",
    "Î¸ = parameters\n",
    "\n",
    "ğ¿\n",
    "(\n",
    "ğœƒ\n",
    ") = loss function\n",
    "\n",
    "Optimization controls how fast, how stable, and how accurately a neural network learns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c798f916-1afe-49ff-8032-3dfe019758f8",
   "metadata": {},
   "source": [
    "# 1. Batch Gradient Descent\n",
    "Definition\n",
    "\n",
    "Batch Gradient Descent calculates the gradient of the loss function using the entire training dataset and updates the weights once per epoch.\n",
    "\n",
    "How it Works\n",
    "\n",
    "Take all training samples\n",
    "\n",
    "Compute total loss\n",
    "\n",
    "Compute gradient\n",
    "\n",
    "Update weights once per epoch\n",
    "\n",
    "Formula\n",
    "ğ‘¤\n",
    "=\n",
    "ğ‘¤\n",
    "âˆ’\n",
    "ğ›¼\n",
    "â‹…\n",
    "1\n",
    "ğ‘›\n",
    "âˆ‘\n",
    "ğ‘–\n",
    "=\n",
    "1\n",
    "ğ‘›\n",
    "âˆ‡\n",
    "ğ¿\n",
    "ğ‘–\n",
    "w=wâˆ’Î±â‹…\n",
    "n\n",
    "1\n",
    "\tâ€‹\n",
    "\n",
    "i=1\n",
    "âˆ‘\n",
    "n\n",
    "\tâ€‹\n",
    "\n",
    "âˆ‡L\n",
    "i\n",
    "\tâ€‹\n",
    "\n",
    "Example\n",
    "\n",
    "If dataset has 10,000 samples, Batch GD uses all 10,000 samples to update weights once.\n",
    "\n",
    "Advantages\n",
    "\n",
    "âœ” Smooth and stable convergence\n",
    "âœ” Guaranteed convergence for convex problems\n",
    "\n",
    "Disadvantages\n",
    "\n",
    "âŒ Very slow for large datasets\n",
    "âŒ High memory usage\n",
    "âŒ Not suitable for deep learning with big data\n",
    "\n",
    "When to Use\n",
    "\n",
    "Small datasets\n",
    "\n",
    "Simple models\n",
    "\n",
    "Theoretical understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cbcaa4-b476-44ab-9e6b-f76b6640ad91",
   "metadata": {},
   "source": [
    "# Loss function (MSE):\n",
    "\n",
    "ğ¿(ğ‘¤)=1/ğ‘›âˆ‘n.i=1/(ğ‘¦ğ‘–âˆ’ğ‘¦^ğ‘–)2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb81f2c-6157-4024-a46d-9040ce044b1b",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD)\n",
    "Definition\n",
    "\n",
    "SGD updates weights using only one training sample at a time.\n",
    "\n",
    "How it Works\n",
    "\n",
    "Pick one data point\n",
    "\n",
    "Compute loss\n",
    "\n",
    "Compute gradient\n",
    "\n",
    "Update weights immediately\n",
    "\n",
    "Formula\n",
    "ğ‘¤\n",
    "=\n",
    "ğ‘¤\n",
    "âˆ’\n",
    "ğ›¼\n",
    "â‹…\n",
    "âˆ‡\n",
    "ğ¿\n",
    "ğ‘–\n",
    "w=wâˆ’Î±â‹…âˆ‡L\n",
    "i\n",
    "\tâ€‹\n",
    "\n",
    "Example\n",
    "\n",
    "If dataset has 10,000 samples, SGD updates weights 10,000 times per epoch.\n",
    "\n",
    "Advantages\n",
    "\n",
    "âœ” Very fast updates\n",
    "âœ” Can escape local minima\n",
    "âœ” Low memory usage\n",
    "\n",
    "Disadvantages\n",
    "\n",
    "âŒ Noisy updates\n",
    "âŒ Loss fluctuates heavily\n",
    "âŒ Hard to converge exactly\n",
    "\n",
    "When to Use\n",
    "\n",
    "Online learning\n",
    "\n",
    "Very large datasets\n",
    "\n",
    "Real-time applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8382ef-bad4-4936-ae0b-0e340628ef16",
   "metadata": {},
   "source": [
    "# Mini-Batch Gradient Descent â­ (Most Used)\n",
    "Definition\n",
    "\n",
    "Mini-batch Gradient Descent updates weights using a small batch of samples (e.g., 16, 32, 64).\n",
    "\n",
    "How it Works\n",
    "\n",
    "Divide dataset into mini-batches\n",
    "\n",
    "Compute gradient for each batch\n",
    "\n",
    "Update weights after each batch\n",
    "\n",
    "Formula\n",
    "ğ‘¤\n",
    "=\n",
    "ğ‘¤\n",
    "âˆ’\n",
    "ğ›¼\n",
    "â‹…\n",
    "1\n",
    "ğ‘š\n",
    "âˆ‘\n",
    "ğ‘–\n",
    "=\n",
    "1\n",
    "ğ‘š\n",
    "âˆ‡\n",
    "ğ¿\n",
    "ğ‘–\n",
    "w=wâˆ’Î±â‹…\n",
    "m\n",
    "1\n",
    "\tâ€‹\n",
    "\n",
    "i=1\n",
    "âˆ‘\n",
    "m\n",
    "\tâ€‹\n",
    "\n",
    "âˆ‡L\n",
    "i\n",
    "\tâ€‹\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "ğ‘š\n",
    "m = batch size\n",
    "\n",
    "Example\n",
    "\n",
    "Dataset = 10,000 samples\n",
    "Batch size = 100\n",
    "â¡ 100 updates per epoch\n",
    "\n",
    "Advantages\n",
    "\n",
    "âœ” Faster than Batch GD\n",
    "âœ” More stable than SGD\n",
    "âœ” Efficient GPU usage\n",
    "âœ” Best balance of speed & accuracy\n",
    "\n",
    "Disadvantages\n",
    "\n",
    "âŒ Needs batch-size tuning\n",
    "\n",
    "When to Use\n",
    "\n",
    "Deep learning models\n",
    "\n",
    "CNNs, RNNs, Transformers\n",
    "\n",
    "Large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e9c141-c790-495d-9f97-ae72ac4b0272",
   "metadata": {},
   "source": [
    "# Advanced Optimization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b76286f-95da-42f3-a7f3-c91d22a32c7f",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "Problem Solved:\n",
    "\n",
    "Reduces oscillations\n",
    "\n",
    "Speeds up convergence\n",
    "\n",
    "Idea:\n",
    "\n",
    "Uses past gradients to guide updates\n",
    "\n",
    "vtâ€‹=Î²vtâˆ’1â€‹+Î±âˆ‡L\n",
    "\n",
    "Î¸=Î¸âˆ’vtâ€‹\n",
    "\n",
    "Advantage:\n",
    "\n",
    "âœ” Faster training\n",
    "âœ” Smooth updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed61ee44-e41b-4e4b-8947-65c2d1ab57e9",
   "metadata": {},
   "source": [
    "## Nesterov Accelerated Gradient (NAG)\n",
    "Improvement over Momentum:\n",
    "\n",
    "Looks ahead before updating\n",
    "\n",
    "ğ‘£\n",
    "ğ‘¡\n",
    "=\n",
    "ğ›½\n",
    "ğ‘£\n",
    "ğ‘¡\n",
    "âˆ’\n",
    "1\n",
    "+\n",
    "ğ›¼\n",
    "âˆ‡\n",
    "ğ¿\n",
    "(\n",
    "ğœƒ\n",
    "âˆ’\n",
    "ğ›½\n",
    "ğ‘£\n",
    "ğ‘¡\n",
    "âˆ’\n",
    "1\n",
    ")\n",
    "v\n",
    "t\n",
    "\tâ€‹\n",
    "\n",
    "=Î²v\n",
    "tâˆ’1\n",
    "\tâ€‹\n",
    "\n",
    "+Î±âˆ‡L(Î¸âˆ’Î²v\n",
    "tâˆ’1\n",
    "\tâ€‹\n",
    "\n",
    ")\n",
    "Advantage:\n",
    "\n",
    "âœ” Better convergence\n",
    "âœ” Less overshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a52371-10de-4935-b5b3-747847a6b49d",
   "metadata": {},
   "source": [
    "## AdaGrad (Adaptive Gradient)\n",
    "Idea:\n",
    "\n",
    "Adapts learning rate for each parameter\n",
    "\n",
    "ğœƒ\n",
    "=\n",
    "ğœƒ\n",
    "âˆ’\n",
    "ğ›¼\n",
    "ğº\n",
    "+\n",
    "ğœ–\n",
    "âˆ‡\n",
    "ğ¿\n",
    "Î¸=Î¸âˆ’\n",
    "G+Ïµ\n",
    "\tâ€‹\n",
    "\n",
    "Î±\n",
    "\tâ€‹\n",
    "\n",
    "âˆ‡L\n",
    "\n",
    "ğº\n",
    "G = sum of squared gradients\n",
    "\n",
    "Advantage:\n",
    "\n",
    "âœ” Good for sparse data\n",
    "\n",
    "Disadvantage:\n",
    "\n",
    "âŒ Learning rate becomes very small over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987196b6-e759-42c0-8f33-a392b9352b1e",
   "metadata": {},
   "source": [
    "## Adam (Adaptive Moment Estimation) â­ (Most Popular)\n",
    "Combines:\n",
    "\n",
    "Momentum + RMSProp\n",
    "\n",
    "Uses:\n",
    "\n",
    "First moment (mean)\n",
    "\n",
    "Second moment (variance)  \n",
    "\n",
    "mtâ€‹=Î²1â€‹mtâˆ’1â€‹+(1âˆ’Î²1â€‹)g\n",
    "\n",
    "vtâ€‹=Î²2â€‹vtâˆ’1â€‹+(1âˆ’Î²2â€‹)g2\n",
    "\n",
    "Î¸=Î¸âˆ’Î±/vt+emtâ€‹\n",
    "\n",
    "Î±=0.001\n",
    "ğ›½1=0.9\n",
    "ğ›½2=0.999\n",
    "\n",
    "Advantages:\n",
    "\n",
    "âœ” Fast convergence\n",
    "âœ” Works well for most problems\n",
    "âœ” Default choice in deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9432596a-6bba-4fe7-8960-ff7d44bcaa85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd78889-3368-44ed-b47d-53eb60471f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e76488e-3355-4256-85c5-14a72b0f48cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b83e31-92c7-4aa6-b311-6885d01d84b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c44dc-2981-4438-83fe-7888b10c5bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ceaba7-4c70-43d7-a48e-9329f928f611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
