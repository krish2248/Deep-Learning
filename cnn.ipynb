{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "955cf2ea-5263-41a5-9d04-cde59476bc25",
   "metadata": {},
   "source": [
    "# cnn\n",
    "cnn is type of deep learning used in supervised machine learning.\n",
    "\n",
    "it is designed to learn from labelled data, typically for task like image classification, object detection and more, by using multiple to capture complex patter in the data.\n",
    "\n",
    "Convolutional Neural Networks (ConvNets or CNNs) are a category of Neural Networks that have proven very effective in areas such as image recognition and classification. ConvNets have been successful in identifying faces, objects and traffic signs apart from powering vision in robots and self driving cars.\n",
    "\n",
    "\n",
    "A Convolutional Neural Network (CNN) is comprised of one or more convolutional layers (often with a subsampling step) and then followed by one or more fully connected layers as in a standard multilayer neural network. The architecture of a CNN is designed to take advantage of the 2D structure of an input image (or other 2D input such as a speech signal). This is achieved with local connections and tied weights followed by some form of pooling which results in translation invariant features. Another benefit of CNNs is that they are easier to train and have many fewer parameters than fully connected networks with the same number of hidden units. In this article we will discuss the architecture of a CNN and the back propagation algorithm to compute the gradient with respect to the parameters of the model in order to use gradient based optimization.\n",
    "\n",
    "## there are three layer\n",
    "### convolution layer\n",
    "### pooling layer\n",
    "### fully connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4546e0-5b21-4fa3-9510-99d516d5c0a1",
   "metadata": {},
   "source": [
    "### convolution layer \n",
    "we apply filter to extract emage\n",
    "\n",
    "apply filter to each row of data and getting (4x4) image\n",
    "here we notice that we loss our padding because data is very important and we dont want to loss our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a9f779-edd1-416b-96ea-ea3180d6a767",
   "metadata": {},
   "source": [
    "### padding layer\n",
    "\n",
    "* zeros\n",
    "* ones\n",
    "* nearest value\n",
    "now, we apply filter insted of original data to padding data.\n",
    "after it perform activation function and convolutted image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e374ec9e-8247-48cf-a39b-13d29839adae",
   "metadata": {},
   "source": [
    "### pooling layer\n",
    "in pooling layer we apply pooling on convolutted image to get max, min or avg pixcel information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d074f0c2-2089-4c9d-9be0-c6a0042138da",
   "metadata": {},
   "source": [
    "### fully connected layer\n",
    "which output image we get we converted into flatten layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8ce4e7-0d57-4037-b9a6-1dbe2aee5afc",
   "metadata": {},
   "source": [
    "# application\n",
    "* image classification\n",
    "* pertern recognition\n",
    "* video analysis\n",
    "* object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b910c7a-f88c-4f32-9cf0-dab7860fdd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense  # Extract image features (edges, shapes), Reduce image size, Convert 2D → 1D, Fully connected neural layer\n",
    "from tensorflow.keras.datasets import mnist  # Loads handwritten digit images (0–9), Each image is 28×28 pixels, Used for CNN practice & learning\n",
    "from tensorflow.keras.utils import to_categorical  # Converts class labels into one-hot encoded format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "285dba0e-14b2-4b97-92f3-9ea38c9f3350",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()  # Images → 28×28 pixels, Labels → 0 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79fdefe7-ca25-46af-9ecc-fbde24653af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 28, 28, 1) # (number_of_samples, height, width, channels) # -1 means, “Python, calculate this number automatically”\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5856160c-ee15-4330-85ae-a0a8c833efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0  # normalization (feature scaling) of image data, Image pixel values originally range from 0 to 255, Dividing by 255.0 scales values to 0 → 1\n",
    "X_test = X_test / 255.0   # 28 × 28 → image size, 1 → grayscale image (black & white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2bfaf2d-f94a-4de1-8d0d-5d78316dde0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01669e3c-d0e9-4a6f-8a6b-455cbe43f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Convolution Layer\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)))\n",
    "\n",
    "# Pooling Layer\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Flatten Layer\n",
    "model.add(Flatten())  # Converts 2D feature maps → 1D vector\n",
    "\n",
    "# Fully Connected Layer         \n",
    "model.add(Dense(128, activation='relu'))  # Dense → every neuron is connected to all neurons of previous layer, 128 → number of neurons, ReLU → activation function\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "313f3032-597b-4060-ac51-98d92233d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(           # compile It prepares the model for training, defines the optimizer, loss function, and evaluation metric for training the model.\n",
    "    optimizer='adam',    # How the model updates weights, Adam = fast & smart optimizer, Combines momentum + adaptive learning rate, Most commonly used\n",
    "    loss='categorical_crossentropy',  # How wrong the model is, Used for multi-class classification, Works with one-hot encoded labels\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8054a2a1-6200-4641-bcc4-369067a92a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 22ms/step - accuracy: 0.9091 - loss: 0.2982 - val_accuracy: 0.9808 - val_loss: 0.0594\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 28ms/step - accuracy: 0.9833 - loss: 0.0548 - val_accuracy: 0.9842 - val_loss: 0.0474\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18ms/step - accuracy: 0.9900 - loss: 0.0317 - val_accuracy: 0.9827 - val_loss: 0.0546\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 18ms/step - accuracy: 0.9934 - loss: 0.0227 - val_accuracy: 0.9844 - val_loss: 0.0462\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 17ms/step - accuracy: 0.9958 - loss: 0.0135 - val_accuracy: 0.9822 - val_loss: 0.0612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2a3a6b53da0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(          # It trains the model using the training data\n",
    "    X_train,        # Input images (training data)\n",
    "    y_train,\n",
    "    epochs=5,         # How many times the model sees the full training data\n",
    "    batch_size=32,    # How many images are trained at one time, 32 images processed together, Smaller batches → stable learning\n",
    "    validation_data=(X_test, y_test)  # Data used to check performance, Model does NOT learn from this, Used only to measure accuracy & loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1ac810a-2bde-4c57-b0e8-fab160f347d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9782 - loss: 0.0793\n",
      "Test Accuracy: 0.982200026512146\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)  # Uses test images Model does not learn, Gives:Loss → error, Accuracy → correct predictions\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea7efffe-fc0a-4d7b-ac45-08d4cf63803a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "Predicted digit: 7\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)  # Model looks at test images, Gives probabilities for each class (0–9)\n",
    "print(\"Predicted digit:\", pred[0].argmax())  # pred[0] Takes the first test image prediction, argmax() Finds the index of the highest probability, Softmax gives probabilities Highest probability = predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a88f63-cee7-4fd6-81b6-4844e00c16bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95510e25-c0b1-4453-9d3d-3cf73173125b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e03819-f7c2-4f80-9e05-93545401bb05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8cc2b3-cd0c-4fe5-abd1-13c1f94f590e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a313e0-1a19-4c5e-87a2-b6e1aa029f60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
